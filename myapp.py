import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage
from zhipuai_embedding import ZhipuAIEmbeddings
from langchain_community.vectorstores import Chroma
from zhipuai_llm import ZhipuaiLLM
from operator import itemgetter
from typing import List
from langchain_core.documents import Document
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import os
api_key = 'your api key'
os.environ['ZHIPUAI_API_KEY'] = api_key
# åˆå§‹åŒ–æ¨¡å‹å’Œå‘é‡æ•°æ®åº“ï¼ˆç¼“å­˜é¿å…é‡å¤åˆå§‹åŒ–ï¼‰
@st.cache_resource
def init_rag_chain():
    zhipuai_model = ZhipuaiLLM(model_name="glm-4-plus", temperature=0.1)
    embedding = ZhipuAIEmbeddings()
    persist_directory = 'path/to/myAgent/mydb'
    vectordb = Chroma(
        embedding_function=embedding,
        persist_directory=persist_directory
    )
    retriever = vectordb.as_retriever(search_kwargs={"k": 4})
    
    def format_docs(docs: List[Document]) -> str:
        return "\n\n".join(f"[{i+1}] {d.page_content}" for i, d in enumerate(docs))

    # åˆå§‹åŒ–è®°å¿†åº“ï¼ˆå®é™…éƒ¨ç½²å»ºè®®ä½¿ç”¨Redisç­‰æŒä¹…åŒ–å­˜å‚¨ï¼‰
    session_store = {}

    def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
        if session_id not in session_store:
            session_store[session_id] = InMemoryChatMessageHistory()
        return session_store[session_id]

    # æ„å»ºæç¤ºæ¨¡æ¿
    contextual_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯èµ„æ·±å¤§æ¨¡å‹ç³»ç»Ÿå·¥ç¨‹å¸ˆã€‚è¯·ç»¼åˆã€çŸ¥è¯†åº“ã€‘ä¸å¯¹è¯å†å²ï¼Œç»™å‡ºå‡†ç¡®ã€ç®€æ´ã€æ¡ç†æ¸…æ™°çš„å›ç­”ã€‚\nã€çŸ¥è¯†åº“ã€‘\n{context}"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "å½“å‰é—®é¢˜ï¼š{input}")
    ])

    # æ„å»ºåŸºç¡€RAGé“¾
    base_rag_chain = (
        {
            "context": itemgetter("input") | retriever | format_docs,
            "input": itemgetter("input"),
            "history": itemgetter("history"),
        }
        | contextual_prompt
        | zhipuai_model
    )

    # æ·»åŠ å†å²è®°å¿†
    conversational_chain = RunnableWithMessageHistory(
        base_rag_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="history",
    )
    return conversational_chain

# æµå¼å“åº”ç”Ÿæˆå™¨
def generate_response(chain, input_text, session_id):
    config = {"configurable": {"session_id": session_id}}
    # æµå¼è¾“å‡º
    for chunk in chain.stream({"input": input_text}, config=config):
        yield chunk.content

# Streamlitåº”ç”¨ç•Œé¢
def main():
    # 1. è®¾ç½®é¡µé¢é…ç½®ï¼ˆåº”ç”¨åç§°ä¿®æ”¹ï¼‰
    st.set_page_config(
        page_title="å¤§æ¨¡å‹çŸ¥è¯†é—®ç­”åŠ©æ‰‹",
        page_icon="ğŸ¤–",
        layout="centered"
    )
    
    # 2. æ·»åŠ å³ä¸‹è§’åŠŸèƒ½æ ‡æ³¨ï¼ˆä½¿ç”¨CSSå›ºå®šä½ç½®ï¼‰
    st.markdown("""
    <style>
        .feature-badge {
            position: fixed;
            right: 20px;
            bottom: 20px;
            background: rgba(0,0,0,0.7);
            color: white;
            padding: 8px 12px;
            border-radius: 12px;
            font-size: 0.8rem;
            z-index: 1000;
        }
    </style>
    <div class="feature-badge">
        âœ… çŸ¥è¯†æ£€ç´¢ï¼ˆå¤§æ¨¡å‹åŸºç¡€å®Œæ•´ç‰ˆ.pdfï¼‰<br>
        âœ… å¤šè½®å¯¹è¯<br>
        âœ… æµå¼è¾“å‡º
    </div>
    """, unsafe_allow_html=True)
    
    # 3. ä¸»ç•Œé¢æ ‡é¢˜
    st.title("ğŸ¤– å¤§æ¨¡å‹çŸ¥è¯†é—®ç­”åŠ©æ‰‹")
    
    # 4. åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # 5. åˆå§‹åŒ–å¯¹è¯é“¾
    if "chain" not in st.session_state:
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–çŸ¥è¯†åº“ç³»ç»Ÿ..."):
            st.session_state.chain = init_rag_chain()
    
    # 6. å›ºå®šsession_id
    session_id = "user_123"
    
    # 7. æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # 8. ç”¨æˆ·è¾“å…¥å¤„ç†
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # å‡†å¤‡AIå›å¤åŒºåŸŸ
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            # æµå¼è·å–å“åº”
            for chunk in generate_response(st.session_state.chain, prompt, session_id):
                full_response += chunk
                response_placeholder.markdown(full_response + "â–Œ")
            
            # æœ€ç»ˆæ˜¾ç¤ºï¼ˆå»æ‰å…‰æ ‡ï¼‰
            response_placeholder.markdown(full_response)
        
        # æ·»åŠ AIå›å¤åˆ°å†å²
        st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()